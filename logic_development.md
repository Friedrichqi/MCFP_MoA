I am designing a Scheduling and Hierarchical Resource Management algorithm for serving multi-agent system with vllm on resource-constrained platforms:

Background:
1. Currently we have N GPUs, each of it owns vram HBM Memory. We should define them as classes, which own gpu_id, vram_total_MB, \alpha (maximum percentage of slept model weights on it), state (STABLE, UNSTABLE), resident (a dict of model_id->model state(ACTIVE, SLEPT)), pid_by_model_id (model_id->active model's pid on this GPU), url_by_model_id, last_used.
2. For each model, we should maintain a modelcard, which contains its model_id, minimum number of tp to serve it, t_wake_s (activating time from slept mode), t_sleep_s (sleeping it from active state), t_load_s (loading it from scratch), t_offload_s (offloading/killing it completely), slept_mem_tp1_MB (size of slept models when serving it with tp1 in MB), slept_mem_tpg1_MB (size of slept models when serving it with tp>1 in MB on one specific GPU, because it would be the same on every GPU). 
3. For each instance, it should have instance_id, model_id, base_url, port, state (LOADING, ACTIVE, SLEPT, VANISHING), pid_by_gpu, vram_by_gpu, created_time, last_used.
4. Each job is a DAG described by adjacent lists. The DAG represents the interdependency between agents (LLMs). Each edge between different agents means the output from previous agents should be fed into next agents. If an agent has multiple predecessors, then the agent's input should be the concatenated output of all predecessors. Different agents can share the same LLMs, so you should distinguish bewteen agents in DAG if they use the same LLM. Once a job is submitted to the online proxy_server, we should break down edges of it (agent requests) and put them in potential list.


Request_Scheduler:
1. For every \delta_{t1} seconds (by default 1s), we relocate requests whose in_degree==0 into waiting_for_scheduling list and record its arrival time into waiting_for_scheduling.
2. For each of the request in waiting_for_scheduling list, we should iterate all ACTIVE instances and figure out if the requested agent has been served. If it's served, then send the request to the active instance that has the minimum draining latency (use vllm host's metrics endpoint, draining latency = "vllm:num_requests" * "vllm:e2e_request_latency_seconds_sum" / "vllm:e2e_request_latency_seconds_count").

GPU_Scheduler:
1. For every \delta_{t2} seconds (by default 5s), we rerun the min-cost-flow algorithm to derive the optimum GPU configuration for current workflow. We define the flow graph into 4 layers.
2. The 1st layer is the source, which is connected to all nodes in the 2nd layer, the GPU layer. Only those GPUs whose state is stable can be added to this layer, because we can only load/activate a model on it when it's idle or stably serving an instance. The flow of edges connecting layer1 and layer2 is 1, indicating that one GPU can only be assigned for one single time. The cost of edges connecting layer1 and layer2 is draining latency of current GPU. For example, the running+waiting list of GPU_i is l_i and average serving latency of GPU_i is t_i, then the draining latency of GPU_i should be l_i \times t_i, which is the same with Request_Scheduler (use vllm host's metrics endpoint, draining latency = "vllm:num_requests" * "vllm:e2e_request_latency_seconds_sum" / "vllm:e2e_request_latency_seconds_count").
3. The third layer should be the all the required models in waiting_for_scheduling and potential list except for currently loading models. The reason why we exclude currently loading models is that we desire to avoid loading the same model again and again when it's scheduled for loading in previous scheduling cycles but hasn't completed the loading process yet. And each model in layer3 should be copied num of GPUs in layer2 times, because different GPUs have different slept-model configurations, which can influence the cost of edges bewteen layer3 and layer4. Each GPU in layer2 will connect to one copy of models in layer3. These edges have a flow of 1 and a cost of switch penalty and sleep/offload cost. The sleep/offload cost of edge that connects GPU_i and Model_j (all copies are uniformly stated as Model_j here and in the following) should be: case1: current slept weights on GPU_i + slept_mem of currently active model on GPU_i + slept_mem_tp1_MB of Model_j <= \alpha of GPU_i * vram_total_MB of GPU_i, we can sleep currently active model with a cost of t_sleep_s of it on GPU_i when trying to serve Model_j. case2: current slept weights on GPU_i + slept_mem of currently active model on GPU_i + slept_mem_tp1_MB of Model_j > \alpha of GPU_i * vram_total_MB of GPU_i, now we have to offload a model to meet this vram capacity restriction. We count the needed times of currently slept/active models on GPU_i in waiting_for_scheduling and potential list and multiply it with the average serving latency of each instance separately. So, the offload cost should be t_offload_s of the model with lowest cost in this case. The switch penalty of edge that connect GPU_i and Model_j should be correspondingly divided into the same two cases. The switch penalty in case1 should be the needed times of currently active model on GPU_i in waiting_for_scheduling and potential list \times t_wake_s of it and in case2, it should be the needed times of currently active model in waiting_for_scheduling and potential list \times t_load_s of it.
4. The fourth layer is the sink. The edges from models (layer3) to sink (layer4) doesn't have flow restrictions (can be inf) for we can use several GPUs to serve one single model (TP) or several GPUs serve multi-instances of the same model (DP). The cost of edges from Model_j to sink should be loading/activating cost - gross waiting cost. Case1: This copy of Model_j is connected to GPU_i, where there isn't an existing slept Model_j instance on GPU_i, then the loading cost is time_load_s of Model_j; Case2: there's existing slept weight of Model_j on GPU_i, the activating cost is time_wake_s of Model_j. Gross waiting cost of Model_j is the sum of all waiting_time (current_time - arriving_time) of requests in waiting_for_scheduling list that require Model_j.
5. Run the min-cost-flow algorithm to derive the reconfiguration strategy that achieves largest flow amount with lowest total cost (highest gpu utilization rate with lowest end2end latency).
6. Serve/sleep/wake up/offload model instances basing on the reconfiguration strategy given by min-cost-flow algorithm and update modelcard correspondingly with EMA.
7. Update all GPUs' state and instances' state.

Here's the entire workflow. Please help me conclude and synthesize for a more straightforward and comprehensive logic in markdown file named logic_devemopment_revised.md. You should start with problem set up, variable definition with detailed expression, Scheduling Policy for requests and GPU reconfiguration step-by-step.